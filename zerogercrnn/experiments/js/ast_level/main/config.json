{
  "prediction_type": "nt_attention",
  "non_terminals_count": 97,
  "terminals_count": 50002,
  "data_train_limit": 250,
  "data_eval_limit": 250,
  "seq_len": 50,
  "batch_size": 10,
  "learning_rate": 0.1,
  "epochs": 10,
  "decay_after_epoch": 0,
  "decay_multiplier": 0.9,
  "embedding_size": 100,
  "hidden_size": 500,
  "num_layers": 1,
  "dropout": 0.03,
  "weight_decay": 0.01
}